{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b45db8921814c41a09e8ef4c9966592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08fd8416d5274cf5b28b500bd5b93294",
              "IPY_MODEL_9d0359b4d34d4f39805940f8856a79d3",
              "IPY_MODEL_e5923f7423d64c3a85648c931fffb904"
            ],
            "layout": "IPY_MODEL_95ae66cfe069431d9a12a35565855c61"
          }
        },
        "08fd8416d5274cf5b28b500bd5b93294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f17a474492a44038b85f0a5f7a7ce40d",
            "placeholder": "​",
            "style": "IPY_MODEL_7cf57d19da2f4107902c64affb409adf",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9d0359b4d34d4f39805940f8856a79d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42c8eee9d99c4ee2a3249763875d4204",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23c0385a6c5d469b924c34c9522ab799",
            "value": 8
          }
        },
        "e5923f7423d64c3a85648c931fffb904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e546a7ed36d4b549ab4baf410fe30c4",
            "placeholder": "​",
            "style": "IPY_MODEL_e4e8929fcb304f6192fdd2423412c4a0",
            "value": " 8/8 [01:08&lt;00:00,  7.56s/it]"
          }
        },
        "95ae66cfe069431d9a12a35565855c61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f17a474492a44038b85f0a5f7a7ce40d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cf57d19da2f4107902c64affb409adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42c8eee9d99c4ee2a3249763875d4204": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c0385a6c5d469b924c34c9522ab799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e546a7ed36d4b549ab4baf410fe30c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e8929fcb304f6192fdd2423412c4a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering\n",
        "\n",
        "We will exercise prompt engineering using a `text-generation` model via the [ChatHuggingFace](https://python.langchain.com/docs/integrations/chat/huggingface/) API."
      ],
      "metadata": {
        "id": "u7prTY6pSS28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain-huggingface text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2 bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "aO7HEdIVSlAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your token: \")"
      ],
      "metadata": {
        "id": "r-cn-HXgSP0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization\n",
        "\n",
        "Quantization techniques **reduce memory and computational costs** by representing weights and activations with lower-precision data types like 8-bit integers (int8).\n",
        "\n",
        "This enables loading larger models you normally wouldn’t be able to fit into memory, and speeding up inference. Transformers supports the AWQ and GPTQ quantization algorithms and it supports 8-bit and 4-bit quantization with `bitsandbytes`."
      ],
      "metadata": {
        "id": "EstGmAHVXUhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "CVHMce1-SnjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2021/07/qat-training-precision.png\" width=\"600\">\n",
        "\n",
        "Image Source: [nvidia.com/blog](https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/)"
      ],
      "metadata": {
        "id": "KnByTHXeYjkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs=dict(\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,  # Enable sampling (will depend on temperature)\n",
        "        temperature=0.35, # Temperature of 0 is equivalent of no sampling (Greedy)\n",
        "        repetition_penalty=1.03,\n",
        "        return_full_text=False, # Determines whether to return the entire generated text or only the last generated token\n",
        "    ),\n",
        "    model_kwargs={\"quantization_config\": quantization_config},\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ],
      "metadata": {
        "id": "hM8otggtSpxF",
        "outputId": "7d2343d6-d342-45f6-863e-ccd6720c6cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "0b45db8921814c41a09e8ef4c9966592",
            "08fd8416d5274cf5b28b500bd5b93294",
            "9d0359b4d34d4f39805940f8856a79d3",
            "e5923f7423d64c3a85648c931fffb904",
            "95ae66cfe069431d9a12a35565855c61",
            "f17a474492a44038b85f0a5f7a7ce40d",
            "7cf57d19da2f4107902c64affb409adf",
            "42c8eee9d99c4ee2a3249763875d4204",
            "23c0385a6c5d469b924c34c9522ab799",
            "4e546a7ed36d4b549ab4baf410fe30c4",
            "e4e8929fcb304f6192fdd2423412c4a0"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b45db8921814c41a09e8ef4c9966592"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_pipeline:Setting the `device` argument to None from -1 to avoid the error caused by attempting to move the model that was already loaded on the GPU using the Accelerate module to the same or another device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `temperature`\n",
        "- In short, the lower the temperature, the more deterministic the results in the sense that the highest probable next token is always picked.\n",
        "- Increasing temperature could lead to more randomness, which encourages more diverse or creative outputs.\n",
        "- In terms of application, you might want to use a lower temperature value for tasks like fact-based QA to encourage more factual and concise responses.\n",
        "- For poem generation or other creative tasks, it might be beneficial to increase the temperature value.\n",
        "\n"
      ],
      "metadata": {
        "id": "kRVKSwHaW1UU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*J37qonVPJvKZpzv2\" width=\"600\">\n",
        "\n",
        "Image Source: [How to sample from language models | by Ben Mann | Towards Data Science]"
      ],
      "metadata": {
        "id": "Y-IhHu1OYOz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `max_new_tokens`\n",
        "\n",
        "Specifying a max length helps you prevent long or irrelevant responses and **control costs**.\n"
      ],
      "metadata": {
        "id": "RSG0eluLXilf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkkoXPHCO8Bq"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're a helpful assistant\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# user_input = \"What happens when an unstoppable force meets an immovable object?\"\n",
        "user_input = input(\"Tell the AI something: \")"
      ],
      "metadata": {
        "id": "F5uT7bvTZPyC",
        "outputId": "c1d5105d-e248-4f2f-a286-6337ae8d5076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tell the AI something: Who are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(HumanMessage(content=user_input))"
      ],
      "metadata": {
        "id": "0OEkSp4uZLKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "id": "cIdqyezMZrrK",
        "outputId": "af186259-e9cd-4d04-e934-af3c4deb7c49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"You're a helpful assistant\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg = chat_model.invoke(messages)"
      ],
      "metadata": {
        "id": "OxFcNQlXZmt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_msg.content)"
      ],
      "metadata": {
        "id": "R3QlvJbQTAhg",
        "outputId": "38f39cbf-54dc-4077-e5f7-ef6bab280c04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm not a physical being, but rather a program designed to assist and provide information. I don't have a physical body, feelings, or personal opinions. My primary function is to help answer your questions and provide accurate and useful information based on the data available to me. I'm here to serve you and make your life easier, so please don't hesitate to ask me anything!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your Turn\n",
        "\n",
        "**Exercise 1**: Experiment with different prompts to achieve different tasks:\n",
        "\n",
        "1. Summarization\n",
        "2. Translation\n",
        "3. Classification\n",
        "4. Get creative and make up your own task\n",
        "\n",
        "Instructions can be as long as you'd like. Be clear about:\n",
        "- The task you want to be achieved by the LLM\n",
        "- Any constraints you want the LLM to take care of\n",
        "- The format, style, tone, and phrasing of the response you want from the LLM\n",
        "\n",
        "Hint: you need to change the `SystemMessage(content=\">>INSTRUCTIONS<<\")` to include your instructions."
      ],
      "metadata": {
        "id": "WljqeJMdTL57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE DOWN HERE\n",
        "messages = [\n",
        "    SystemMessage(content=\"PIF is driving the growth of new sectors, companies and jobs, as a catalyst of Vision 2030. As a global impactful investor, the Fund has a world-class investment portfolio with a focus on sustainable investments, both domestically and internationally.\"),\n",
        "]"
      ],
      "metadata": {
        "id": "fwvVliTOTfB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# user_input = \"What happens when an unstoppable force meets an immovable object?\"\n",
        "user_input = input(\"Tell the AI something: \")"
      ],
      "metadata": {
        "id": "rO18ZY9OBmCj",
        "outputId": "a0876019-6f1b-4420-f972-c3adb77a0eca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tell the AI something: Can you give me a very short summary of the text?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(HumanMessage(content=user_input))"
      ],
      "metadata": {
        "id": "HVHQnzbkBovz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg = chat_model.invoke(messages)"
      ],
      "metadata": {
        "id": "OwAJvJb_Bqch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_msg.content)"
      ],
      "metadata": {
        "id": "74CRG76YBsKo",
        "outputId": "40329971-9ca1-40fc-9845-02a87102b80e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Public Investment Fund (PIF) in Saudi Arabia is promoting the growth of new industries, businesses, and jobs as part of the country's Vision 2030 plan. The PIF is a global investor that focuses on sustainable investments, both locally and internationally, and is playing a significant role in driving economic development.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Exercise 2**: use a different LLM and compare the results by eye.\n",
        "\n",
        "Hint: you need to change `model_id` to be some `text-generation` model from [HuggingFace Hub](https://huggingface.co/models?pipeline_tag=text-generation)."
      ],
      "metadata": {
        "id": "gTxz7nE1VFg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE DOWN HERE"
      ],
      "metadata": {
        "id": "6VcFWzTGVGI3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}